{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Linear Regression \n",
    "---\n",
    "\n",
    "This chapter is about linear regression, a very simple, yet powerful, approach to supervised learning for predicting a quantitative respone. In this chapter, we review some key ideas underlying the linear regression model, as well as the <strong>least squares</strong> approach, that is commonly used to <strong>fit</strong> a model. \n",
    "\n",
    "## 3.1 Simple Linear Regression \n",
    "SLR lives up to its name: it's a straightforward approach for predicting a response <strong>$Y$</strong> on the basis of a <strong>single</strong> predictor $X$. We assume there is approximately a linear relationship between the two. Mathematically it's written as: <br>\n",
    " $$\n",
    "\\begin{equation}\n",
    "  Y \\approx \\beta_0 + \\beta_1X\n",
    "\\label{eq:SLR}\n",
    "\\tag{3.1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\"$\\approx$\" is read as approximately. We use it because of the error associated to predicting $Y$. <br> We can describe (3.1) by saying that we are regressing $Y$ onto $X$. For example, $X$ may represent TV ads and $Y$ represents sales. <br>\n",
    " $$\n",
    "\\begin{equation}\n",
    "  sales \\approx \\beta_0 + \\beta_1TV\n",
    "\\label{eq:sales}\n",
    "\\\n",
    "\\end{equation}\n",
    "$$ <br>\n",
    "\n",
    "In (3.1), $\\beta_0$ and $\\beta_1$ are two <strong>unkown</strong> constants that represent the *intercept* and *slope* terms in the linear model. They're known as *coefficients* or *parameters*. <br>\n",
    "We use our **training data** to produce $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ for the **model** coefficients. We use this model to predict future sales based on a particular TV ad by computing: <br>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\widehat{y}  = \\hat{\\beta_0} + \\hat{\\beta_1}x\n",
    "\\label{eq:model1}\n",
    "\\tag{3.2}\n",
    "\\end{equation}\n",
    "$$ \n",
    "<br>\n",
    "Where $\\widehat{y}$ indicates a prediction of $Y$ on the basis of $X$ = $x$. The *hat* symbol, $\\widehat{}$,  denotes the estimated value for an unknown parameter or coefficient, or the predicted value of the response.\n",
    "\n",
    " \n",
    "### 3.1.1 Estimating the Coefficients\n",
    "In practice $\\beta_0$ and $\\beta_1$ are unknown. We therefore must use the data to estimate those two. <br>\n",
    "Let n observation pairs:\n",
    "$$(x_1, y_1), (x_2, y_2)...,(x_n,y_n)$$ <br>\n",
    "The advertising data has n = 200. In other words, for every market in the 200 markets we have a TV budget and the product sales. Our goal here is to obtain the estimate coefficients $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ for a line which is the **closest** to all the 200 data points. Take a look at what that line would look like: \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img width=\"400\" height=\"300\" src=images/ad_data.png>\n",
    "</p>\n",
    "<br>\n",
    "So that for every $i$-*th* observation in the dataset:\n",
    "$$\n",
    "\\begin{equation}\n",
    " y_i  \\approx \\hat{\\beta_0} + \\hat{\\beta_1}x_i\n",
    "\\label{eq:ith}\n",
    "\\\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "By far the most common approach to measure closeness invovles minimizing the *least squares* criterion. Other approaches are discussed in Chapter 6. <br>\n",
    "The fit is found by minimizing the sum of squared errors. Every grey line in the figure is an error and a 'compromise' is reached by averaging their squares. \n",
    "Ok, time for some fun! <br>\n",
    "let $\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1}x_i$ the prediction of $Y$ based on $i$th value of $X$. Then $e_i = y_i - \\hat{y_i}$ which is the *residual* - or in other words, the difference between the actual value and the predicted one usiong our linear model. The *residual sum of squares* (RSS) is then <br>\n",
    "$$RSS = e^2_1 + e^2_2 + ... + e^2_n$$\n",
    "Orrrr\n",
    "$$\n",
    "RSS = (y_1 - \\hat{\\beta_0} - \\hat{\\beta_1}x_1)^2 + (y_2 - \\hat{\\beta_0} - \\hat{\\beta_1}x_2)^2 + ... + (y_n - \\hat{\\beta_0} - \\hat{\\beta_1}x_n)^2\n",
    "$$\n",
    "The *least squares* chooses $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ which minimizes the $RSS$. Using calculus one can prove: <br>\n",
    "\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n",
    ",$$ $\\tag{3.4}$\n",
    "\n",
    "$$\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x},$$\n",
    "\n",
    "\n",
    "### 3.1.2 Assessing the Accuracy of the Coefficient Estimates\n",
    "In chapter 2 we assumed a true relationship between $X$ and $Y$ in the form $Y = f(X) + \\epsilon$ for some unkown $f$, where $\\epsilon$ is mean-zero random error term. If we were to approximate $f$ by a linear function then the relationship is\n",
    "$$Y = \\beta_0 + \\beta_1X + \\epsilon \\tag{3.5}$$\n",
    "Equation (3.5) is the *population regression line* which is the best linear approx to the true relationship. \n",
    "The term $\\epsilon$ is a catch-all for what we miss with this simple model. Since the true relationship is probably not linear, and there may be other variables that cause variation in $Y$. $\\epsilon$ is typically independent of $X$. <br>\n",
    "At first the difference between the population regression line and the least squares line may be subtle and confusing. In practice we don't have access to the first and we are interested in estimating the latter. <br>\n",
    "Suppose we are interested in computing the mean $\\mu$ of some random variable $Y$. Using the n observations we have access to we can estimate $\\mu$. A resonable estimate is $\\widehat{\\mu}$ = $\\bar{y}$ where $\\bar{y}$ is the sample mean. The sample will give us a good estimate of the population mean. In the same way, $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ will give us a good estimate to the coefficients $\\beta_0$ and $\\beta_1$ in the population regression line. <br>\n",
    "The analogy between linear regression and estimation of the mean is an apt one based on the concept of *bias*. On **avergae** we expect $\\widehat{\\mu}$ to equal $\\mu$. That is to say, it can on occasions underestimate or overestimate the population mean. But a huge number of sample means would *exactly* equal the population mean. We say $\\widehat{\\mu}$ is an *unbiased* estimator of $\\mu$. This property of unbiasedness holds also true for our least squares coefficient estimates. If we could average estimates obtained over a huge number of datasets, then the average would be spot on!\n",
    "<br>\n",
    "So, how accurately does a single $\\widehat{\\mu}$ estimate $\\mu$? In general, this is answered by computing the *standard error* of $\\widehat{\\mu}$. The well-known formula \n",
    "$$Var(\\widehat{\\mu}) = SE(\\widehat{\\mu})^2 = \\frac{\\sigma^2}{n}\n",
    ",\\tag{3.7}$$\n",
    "\n",
    "The standard error reflects the amount by which an estimator varies under repeated sampling. Similarely we can compute:\n",
    "\n",
    "$$SE(\\hat{\\beta_0})^2 = {\\sigma}^2[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}],$$\n",
    "\n",
    "$\\tag{3.8}$\n",
    "\n",
    "\n",
    "$$SE(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2},$$\n",
    "\n",
    "$\\sigma^2$ is unkown, but can be estimated from the data. The estimate of $\\sigma$ is known as the *residual standard error* and is given be the formula\n",
    "$$RSE = \\sqrt{RSS/(n-2)}$$\n",
    "Strictly speaking, when $\\sigma^2$ is estimated we should write $\\widehat{SE}(\\hat{\\beta_1})^2$ to indicate an estimation has taken place but for simplicity we will drop the hat here. \n",
    "Standard errors can be used to compute **confidence intervals**. A 95% confidence interval is defined as a range of values such that with 0.95 probability, the range will contain the true value unknown value of the parameter. It has lower and upper limits computed from the sample data. <br> \n",
    "For linear regression, the 95% confidence intervals for the values $\\beta_1$ and $\\beta_0$ take the following forms\n",
    "\n",
    "$$\\hat{\\beta_1} \\pm 2 \\cdot SE(\\hat{\\beta_1}) \\tag{3.9}$$ \n",
    "\n",
    "Also can be written\n",
    "$$[\\hat{\\beta_1} - 2 \\cdot SE(\\hat{\\beta_1}),  \\hat{\\beta_1} + 2 \\cdot SE(\\hat{\\beta_1})] \\tag{3.10}$$\n",
    "\n",
    "Similarly\n",
    "$$\\hat{\\beta_0} \\pm 2 \\cdot SE(\\hat{\\beta_0}) \\tag{3.11}$$\n",
    "\n",
    "Standard errors can also be used to compute *hypothesis tests*.\n",
    "\n",
    "$H_0$: There is no relationship between $X$ and $Y$ $\\tag{3.12}$\n",
    "*Versus* <br>\n",
    "$H_\\alpha$: There is some relationship\n",
    "\n",
    "Mathematically this is written\n",
    "$$H_0: \\beta_1 = 0$$\n",
    "$$H_\\alpha: \\beta_1 \\ne 0$$\n",
    "\n",
    "To test our hypothesis, we need to determine whether $\\hat{\\beta_1}$ is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero. Well, how far is enough? This of course depends on the accuracy of $\\hat{\\beta_1}$!! That is - it depends on $SE(\\hat{\\beta_1})$. If that standard error is small, a small value of $\\hat{\\beta_1}$ may provide strong evidence that $\\beta_1 \\ne 0$. If it's a large number, then $\\hat{\\beta_1}$ must be large in absolute value to draw the same conclusion. In practice, we compute a *t-statistic*, given by\n",
    "\n",
    "$$ t = \\frac{\\hat{\\beta_1}-0}{SE(\\hat{\\beta_1})}\n",
    ",$$ $\\tag{3.14}$\n",
    "\n",
    "which measures the number of standard deviations that $\\hat{\\beta_1}$ is away from 0. The test statistic follows a t distribution with (n-2) degrees of freedom, where n is the total number of observations. <br>\n",
    "The null hypothesis is accepted if the calculated value is such that:\n",
    "$$-t_{\\alpha/2,n-2}<t<t_{\\alpha/2,n-2}$$\n",
    "where $-t_{\\alpha/2,n-2}$ and $t_{\\alpha/2,n-2}$ are the critical values for the two-sided hypothesis. The t-distribution has a bell shape for values of n greater than approximately 30. <br>$t_{\\alpha/2,n-2}$ is the percentile of the t distribution corresponding to a cumulative probability of (1-$\\alpha$/2) and $\\alpha$ is the significance level. We call this probability the **p-value**. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance. Typically the cuttoffs for rejecting the hypothesis are 5 and 1%. When n = 30, these correspond to t-statistics (3.14) of around 2 and 2.75, respectively.\n",
    "\n",
    "\n",
    "\n",
    "### 3.1.3 Assessing the Accuracy of the Model\n",
    "\n",
    "It is natural to want to quantify the extent to which the model fits the data. <br>\n",
    "The quality of a linear regression fit is typically assessed using two related quantities: the *residual standard error* $(RSE)$ and the **$R^2$** statistic.\n",
    "\n",
    "**Residual Standar Error**\n",
    "<br>\n",
    "Even if we knew the true regression line we would not be able to perfectly predict $Y$ from $X$. This is due to the error term $\\epsilon$ in equation (3.5), that is associated with every observation. <br>\n",
    "The **$RSE$** is an estimate of the standard deviation of $\\epsilon$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{1}{n-2}RSS} = \\sqrt{\\frac{1}{n-2}(y_i - \\hat{y_i})^2} \\tag{3.15}$$\n",
    "\n",
    "## 3.2 Multiple Linear Regression \n",
    "### 3.2.1 Estimating the Regression Coefficients \n",
    "### 3.2.2 Some Important Questions\n",
    "\n",
    "## 3.3 Other Considerations in the Regression Model \n",
    "### 3.3.1 Qualitative Predictors \n",
    "### 3.3.2 Extensions of the Linear Model \n",
    "### 3.3.3 Potential Problems \n",
    "## 3.5 Comparison of Linear Regression with K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
